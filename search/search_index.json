{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service \u00b6 Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours Objectives \u00b6 After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices Prerequisites \u00b6 You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh Cleaning up the Workshop \u00b6 Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"About the workshop"},{"location":"#beyond-the-basics-istio-and-ibm-cloud-kubernetes-service","text":"Istio is an open platform to connect, secure, control and observe microservices, also known as a service mesh, on cloud platforms such as Kubernetes in IBM Cloud Kubernetes Service and VMs. With Istio, You can manage network traffic, load balance across microservices, enforce access policies, verify service identity, secure service communication and observe what exactly is going on with your services. YouTube: Istio Service Mesh Explained: In this course, you can see how to install Istio alongside microservices for a simple mock app called Guestbook . When you deploy Guestbook's microservices into an IBM Cloud Kubernetes Service cluster where Istio is installed, you can choose to inject the Istio Envoy sidecar proxies in the pods of certain microservices. Estimated completion time: 2 hours","title":"Beyond the Basics: Istio and IBM Cloud Kubernetes Service"},{"location":"#objectives","text":"After you complete this course, you'll be able to: Download and install Istio in your cluster Deploy the Guestbook sample app Use metrics, logging and tracing to observe services Set up the Istio Ingress Gateway Perform simple traffic management, such as A/B tests and canary deployments Secure your service mesh Enforce policies for your microservices","title":"Objectives"},{"location":"#prerequisites","text":"You must you must have a Pay-As-You-Go, or Subscription IBM Cloud account to complete all the modules in this course. You must have already created a Standard 1.16+ cluster in IBM Cloud Kubernetes Service. FREE Cluster is not supported for this lab You should have a basic understanding of containers, IBM Cloud Kubernetes Service, and Istio. If you have no experience with those, take the following courses: Get started with Kubernetes and IBM Cloud Kubernetes Service Get started with Istio and IBM Cloud Kubernetes Service","title":"Prerequisites"},{"location":"#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"#cleaning-up-the-workshop","text":"Script to uninstall ibmcloud CLI: clean_your_local_machine.sh and unset KUBECONFIG . Script to delete Istio and Guestbook: clean_your_k8s_cluster.sh .","title":"Cleaning up the Workshop"},{"location":"SUMMARY/","text":"Table of contents \u00b6 About this workshop \u00b6 Overview Workshop setup \u00b6 Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application Creating a service mesh with Istio \u00b6 Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Table of contents"},{"location":"SUMMARY/#table-of-contents","text":"","title":"Table of contents"},{"location":"SUMMARY/#about-this-workshop","text":"Overview","title":"About this workshop"},{"location":"SUMMARY/#workshop-setup","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service Exercise 2 - Installing Istio Exercise 3 - Deploying Guestbook sample application","title":"Workshop setup"},{"location":"SUMMARY/#creating-a-service-mesh-with-istio","text":"Exercise 4 - Observe service telemetry: metrics and tracing Exercise 5 - Expose the service mesh with the Istio Ingress Gateway Exercise 6 - Perform traffic management Exercise 7 - Secure your service mesh","title":"Creating a service mesh with Istio"},{"location":"exercise-1/","text":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service \u00b6 You must already have a cluster created . Your cluster must have 3 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.16 or later. Install IBM Cloud Kubernetes Service command line utilities \u00b6 Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Log in to the IBM Cloud CLI. (If you have a federated account, include the --sso flag.) ibmcloud login Access your cluster \u00b6 Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes Clone the lab repo \u00b6 From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/docs This is the working directory for the workshop. You will use the example .yaml files that are located in the workshop/plans directory in the following exercises. Continue to Exercise 2 - Installing Istio \u00b6","title":"Lab 1. Access a Kubernetes cluster"},{"location":"exercise-1/#exercise-1-accessing-a-kubernetes-cluster-with-ibm-cloud-kubernetes-service","text":"You must already have a cluster created . Your cluster must have 3 or more worker nodes with at least 4 cores and 16GB RAM , and run Kubernetes version 1.16 or later.","title":"Exercise 1 - Accessing a Kubernetes cluster with IBM Cloud Kubernetes Service"},{"location":"exercise-1/#install-ibm-cloud-kubernetes-service-command-line-utilities","text":"Download and install the required CLI tools. curl -sL https://ibm.biz/idt-installer | bash Log in to the IBM Cloud CLI. (If you have a federated account, include the --sso flag.) ibmcloud login","title":"Install IBM Cloud Kubernetes Service command line utilities"},{"location":"exercise-1/#access-your-cluster","text":"Learn how to set the context to work with your cluster by using the kubectl CLI, access the Kubernetes dashboard, and gather basic information about your cluster. Set the context for your cluster in your CLI. Every time you log in to the IBM Cloud Kubernetes Service CLI to work with the cluster, you must run these commands to set the path to the cluster's configuration file as a session variable. The Kubernetes CLI uses this variable to find a local configuration file and certificates that are necessary to connect with the cluster in IBM Cloud. a. List the available clusters. ibmcloud ks clusters b. Set an environment variable for your cluster name: export MYCLUSTER = <your_cluster_name> c. Download the configuration file and certificates for your cluster using the cluster-config command. ibmcloud ks cluster config --cluster $MYCLUSTER Get basic information about your cluster and its worker nodes. This information can help you manage your cluster and troubleshoot issues. a. View details of your cluster. ibmcloud ks cluster get --cluster $MYCLUSTER b. Verify the worker nodes in the cluster. ibmcloud ks workers --cluster $MYCLUSTER Validate access to your cluster by viewing the nodes in the cluster. kubectl get nodes","title":"Access your cluster"},{"location":"exercise-1/#clone-the-lab-repo","text":"From your command line, run: git clone https://github.com/IBM/istio101 cd istio101/docs This is the working directory for the workshop. You will use the example .yaml files that are located in the workshop/plans directory in the following exercises.","title":"Clone the lab repo"},{"location":"exercise-1/#continue-to-exercise-2-installing-istio","text":"","title":"Continue to Exercise 2 - Installing Istio"},{"location":"exercise-2/","text":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service \u00b6 In this module, you will use the Managed Istio add-on to install Istio on your cluster. Managed Istio is available as part of IBM Cloud\u2122 Kubernetes Service. The service provides seamless installation of Istio, automatic updates and lifecycle management of control plane components, and integration with platform logging and monitoring tools. Download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .11.4 sh - export PATH = $PWD /istio-1.11.4/bin: $PATH Enable Managed Istio on your IKS cluster: ibmcloud ks cluster addon enable istio --cluster $MYCLUSTER The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: NAME READY STATUS RESTARTS AGE istio-egressgateway-6c966469cc-52t6f 1 /1 Running 0 69s istio-egressgateway-6c966469cc-qq5qd 1 /1 Running 0 55s istio-ingressgateway-7698c7b4f4-69c24 1 /1 Running 0 68 istio-ingressgateway-7698c7b4f4-qttzh 1 /1 Running 0 54s istiod-cbb98c74d-2wvql 1 /1 Running 0 54s istiod-cbb98c74d-kcr4d 1 /1 Running 0 67s NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .11.4 control plane version: 1 .11.4 data plane version: 1 .11.4 ( 4 proxies ) Congratulations! You successfully installed Istio into your cluster. Continue to Exercise 3 - Deploy Guestbook with Istio Proxy \u00b6","title":"Lab 2. Install Istio"},{"location":"exercise-2/#exercise-2-installing-istio-on-ibm-cloud-kubernetes-service","text":"In this module, you will use the Managed Istio add-on to install Istio on your cluster. Managed Istio is available as part of IBM Cloud\u2122 Kubernetes Service. The service provides seamless installation of Istio, automatic updates and lifecycle management of control plane components, and integration with platform logging and monitoring tools. Download the istioctl CLI and add it to your PATH: curl -L https://istio.io/downloadIstio | ISTIO_VERSION = 1 .11.4 sh - export PATH = $PWD /istio-1.11.4/bin: $PATH Enable Managed Istio on your IKS cluster: ibmcloud ks cluster addon enable istio --cluster $MYCLUSTER The install can take up to 10 minutes. Ensure the corresponding pods are all in Running state before you continue. kubectl get pods -n istio-system Sample output: NAME READY STATUS RESTARTS AGE istio-egressgateway-6c966469cc-52t6f 1 /1 Running 0 69s istio-egressgateway-6c966469cc-qq5qd 1 /1 Running 0 55s istio-ingressgateway-7698c7b4f4-69c24 1 /1 Running 0 68 istio-ingressgateway-7698c7b4f4-qttzh 1 /1 Running 0 54s istiod-cbb98c74d-2wvql 1 /1 Running 0 54s istiod-cbb98c74d-kcr4d 1 /1 Running 0 67s NOTE Before you continue, make sure all the pods are deployed and either in the Running or Completed state. If they're in pending state, wait a few minutes to let the installation and deployment finish. Check the version of your Istio: istioctl version Sample output: client version: 1 .11.4 control plane version: 1 .11.4 data plane version: 1 .11.4 ( 4 proxies ) Congratulations! You successfully installed Istio into your cluster.","title":"Exercise 2 - Installing Istio on IBM Cloud Kubernetes Service"},{"location":"exercise-2/#continue-to-exercise-3-deploy-guestbook-with-istio-proxy","text":"","title":"Continue to Exercise 3 - Deploy Guestbook with Istio Proxy"},{"location":"exercise-3/","text":"Exercise 3 - Deploy the Guestbook app with Istio Proxy \u00b6 The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons. Download the Guestbook app \u00b6 Clone the Guestbook app into the workshop directory. git clone https://github.com/IBM/guestbook Navigate into the app directory. cd guestbook/v2 Enable the automatic sidecar injection for the default namespace \u00b6 In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ... Create a Redis database \u00b6 The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d Install the Guestbook app \u00b6 Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-98dd9c654-dz8dq 2 /2 Running 0 30s guestbook-v1-98dd9c654-mgfv6 2 /2 Running 0 30s guestbook-v1-98dd9c654-x8gxx 2 /2 Running 0 30s guestbook-v2-8689f6c559-5ntgv 2 /2 Running 0 28s guestbook-v2-8689f6c559-fpzb7 2 /2 Running 0 28s guestbook-v2-8689f6c559-wqbnl 2 /2 Running 0 28s redis-master-577bc6fbb-zh5v8 2 /2 Running 0 4m47s redis-slave-7779c6f75b-bshvs 2 /2 Running 0 4m46s redis-slave-7779c6f75b-nvsd6 2 /2 Running 0 4m46s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar. Use Watson Tone Analyzer (Optional) \u00b6 Watson Tone Analyzer detects the tone from the words that users enter into the Guestbook app. The tone is converted to the corresponding emoticons. Create Watson Tone Analyzer in your account. ibmcloud resource service-instance-create my-tone-analyzer-service tone-analyzer lite us-south Create the service key for the Tone Analyzer service. This command should output the credentials you just created. You will need the value for apikey & url later. ibmcloud resource service-key-create tone-analyzer-key Manager --instance-name my-tone-analyzer-service If you need to get the service-keys later, you can use the following command: ibmcloud resource service-key tone-analyzer-key Open the analyzer-deployment.yaml and find the env section near the end of the file. Replace YOUR_API_KEY with your own API key, and replace YOUR_URL with the url value you saved before. YOUR_URL should look something like https://gateway.watsonplatform.net/tone-analyzer/api . Save the file. Deploy the analyzer pods and service, using the analyzer-deployment.yaml and analyzer-service.yaml files found in the guestbook/v2 directory. The analyzer service talks to Watson Tone Analyzer to help analyze the tone of a message. Ensure you are still in the guestbook/v2 directory. kubectl apply -f analyzer-deployment.yaml kubectl apply -f analyzer-service.yaml Great! Your guestbook app is up and running. In Exercise 4, you'll be able to see the app in action by directly accessing the service endpoint. You'll also be able to view Telemetry data for the app. Continue to Exercise 4 - Telemetry \u00b6","title":"Lab 3. Deploy sample application"},{"location":"exercise-3/#exercise-3-deploy-the-guestbook-app-with-istio-proxy","text":"The Guestbook app is a sample app for users to leave comments. It consists of a web front end, Redis master for storage, and a replicated set of Redis slaves. We will also integrate the app with Watson Tone Analyzer which detects the sentiment in users' comments and replies with emoticons.","title":"Exercise 3 - Deploy the Guestbook app with Istio Proxy"},{"location":"exercise-3/#download-the-guestbook-app","text":"Clone the Guestbook app into the workshop directory. git clone https://github.com/IBM/guestbook Navigate into the app directory. cd guestbook/v2","title":"Download the Guestbook app"},{"location":"exercise-3/#enable-the-automatic-sidecar-injection-for-the-default-namespace","text":"In Kubernetes, a sidecar is a utility container in the pod, and its purpose is to support the main container. For Istio to work, Envoy proxies must be deployed as sidecars to each pod of the deployment. There are two ways of injecting the Istio sidecar into a pod: manually using the istioctl CLI tool or automatically using the Istio sidecar injector. In this exercise, we will use the automatic sidecar injection provided by Istio. Annotate the default namespace to enable automatic sidecar injection: kubectl label namespace default istio-injection = enabled Validate the namespace is annotated for automatic sidecar injection: kubectl get namespace -L istio-injection Sample output: NAME STATUS AGE ISTIO-INJECTION default Active 271d enabled istio-system Active 5d2h ...","title":"Enable the automatic sidecar injection for the default namespace"},{"location":"exercise-3/#create-a-redis-database","text":"The Redis database is a service that you can use to persist the data of your app. The Redis database comes with a master and slave modules. Create the Redis controllers and services for both the master and the slave. kubectl create -f redis-master-deployment.yaml kubectl create -f redis-master-service.yaml kubectl create -f redis-slave-deployment.yaml kubectl create -f redis-slave-service.yaml Verify that the Redis controllers for the master and the slave are created. kubectl get deployment Output: NAME READY UP-TO-DATE AVAILABLE AGE redis-master 1 /1 1 1 2m16s redis-slave 2 /2 2 2 2m15s Verify that the Redis services for the master and the slave are created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE redis-master ClusterIP 172 .21.85.39 <none> 6379 /TCP 5d redis-slave ClusterIP 172 .21.205.35 <none> 6379 /TCP 5d Verify that the Redis pods for the master and the slave are up and running. kubectl get pods Output: NAME READY STATUS RESTARTS AGE redis-master-4sswq 2 /2 Running 0 5d redis-slave-kj8jp 2 /2 Running 0 5d redis-slave-nslps 2 /2 Running 0 5d","title":"Create a Redis database"},{"location":"exercise-3/#install-the-guestbook-app","text":"Inject the Istio Envoy sidecar into the guestbook pods, and deploy the Guestbook app on to the Kubernetes cluster. Deploy both the v1 and v2 versions of the app: kubectl apply -f ../v1/guestbook-deployment.yaml kubectl apply -f guestbook-deployment.yaml These commands deploy the Guestbook app on to the Kubernetes cluster. Since we enabled automation sidecar injection, these pods will be also include an Envoy sidecar as they are started in the cluster. Here we have two versions of deployments, a new version ( v2 ) in the current directory, and a previous version ( v1 ) in a sibling directory. They will be used in future sections to showcase the Istio traffic routing capabilities. Create the guestbook service. kubectl create -f guestbook-service.yaml Verify that the service was created. kubectl get svc Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE guestbook LoadBalancer 172 .21.36.181 169 .61.37.140 80 :32149/TCP 5d ... Verify that the pods are up and running. kubectl get pods Sample output: NAME READY STATUS RESTARTS AGE guestbook-v1-98dd9c654-dz8dq 2 /2 Running 0 30s guestbook-v1-98dd9c654-mgfv6 2 /2 Running 0 30s guestbook-v1-98dd9c654-x8gxx 2 /2 Running 0 30s guestbook-v2-8689f6c559-5ntgv 2 /2 Running 0 28s guestbook-v2-8689f6c559-fpzb7 2 /2 Running 0 28s guestbook-v2-8689f6c559-wqbnl 2 /2 Running 0 28s redis-master-577bc6fbb-zh5v8 2 /2 Running 0 4m47s redis-slave-7779c6f75b-bshvs 2 /2 Running 0 4m46s redis-slave-7779c6f75b-nvsd6 2 /2 Running 0 4m46s Note that each guestbook pod has 2 containers in it. One is the guestbook container, and the other is the Envoy proxy sidecar.","title":"Install the Guestbook app"},{"location":"exercise-3/#use-watson-tone-analyzer-optional","text":"Watson Tone Analyzer detects the tone from the words that users enter into the Guestbook app. The tone is converted to the corresponding emoticons. Create Watson Tone Analyzer in your account. ibmcloud resource service-instance-create my-tone-analyzer-service tone-analyzer lite us-south Create the service key for the Tone Analyzer service. This command should output the credentials you just created. You will need the value for apikey & url later. ibmcloud resource service-key-create tone-analyzer-key Manager --instance-name my-tone-analyzer-service If you need to get the service-keys later, you can use the following command: ibmcloud resource service-key tone-analyzer-key Open the analyzer-deployment.yaml and find the env section near the end of the file. Replace YOUR_API_KEY with your own API key, and replace YOUR_URL with the url value you saved before. YOUR_URL should look something like https://gateway.watsonplatform.net/tone-analyzer/api . Save the file. Deploy the analyzer pods and service, using the analyzer-deployment.yaml and analyzer-service.yaml files found in the guestbook/v2 directory. The analyzer service talks to Watson Tone Analyzer to help analyze the tone of a message. Ensure you are still in the guestbook/v2 directory. kubectl apply -f analyzer-deployment.yaml kubectl apply -f analyzer-service.yaml Great! Your guestbook app is up and running. In Exercise 4, you'll be able to see the app in action by directly accessing the service endpoint. You'll also be able to view Telemetry data for the app.","title":"Use Watson Tone Analyzer (Optional)"},{"location":"exercise-3/#continue-to-exercise-4-telemetry","text":"","title":"Continue to Exercise 4 - Telemetry"},{"location":"exercise-4/","text":"Exercise 4 - Observe service telemetry: metrics and tracing \u00b6 Challenges with microservices \u00b6 We all know that microservice architecture is the perfect fit for cloud native applications and it increases the delivery velocities greatly. Envision you have many microservices that are delivered by multiple teams, how do you observe the the overall platform and each of the service to find out exactly what is going on with each of the services? When something goes wrong, how do you know which service or which communication among the few services are causing the problem? Istio telemetry \u00b6 Istio's tracing and metrics features are designed to provide broad and granular insight into the health of all services. Istio's role as a service mesh makes it the ideal data source for observability information, particularly in a microservices environment. As requests pass through multiple services, identifying performance bottlenecks becomes increasingly difficult using traditional debugging techniques. Distributed tracing provides a holistic view of requests transiting through multiple services, allowing for immediate identification of latency issues. With Istio, distributed tracing comes by default. This will expose latency, retry, and failure information for each hop in a request. You can read more about how Istio mixer enables telemetry reporting . Configure Istio to receive telemetry data \u00b6 Enable Istio monitoring dashboards, by running these two commands: kubectl patch cm managed-istio-custom -n ibm-operators --type = 'json' -p = '[{\"op\": \"add\", \"path\": \"/data/istio-monitoring\", \"value\":\"true\"}]' kubectl annotate iop -n ibm-operators managed-istio --overwrite version = \"custom-applied-at: $( date ) \" Verify that the Grafana, Prometheus, Kiali and Jaeger add-ons were installed successfully. All add-ons are installed into the istio-system namespace. kubectl get services -n istio-system Obtain the guestbook endpoint to access the guestbook. You can access the guestbook via the external IP for your service as guestbook is deployed as a load balancer service. Get the EXTERNAL-IP of the guestbook service via output below: kubectl get service guestbook -n default Go to this external ip address in the browser to try out your guestbook. This service will route you to either v1 or v2, at random. If you wish to see a different version, you'll need to do a hard refresh ( cmd + shift + r on a mac, or ctrl + f5 on a PC). Alternatively, you can curl the address. Generate a small load to the app, replacing guestbook_IP with the EXTERNAL-IP. for i in { 1 ..40 } ; do sleep 0 .2 ; curl -I http://<guestbook_IP>/ ; done View guestbook telemetry data \u00b6 Jaeger \u00b6 Launch the Jaeger dashboard: istioctl dashboard jaeger From the Services menu, select either the guestbook or analyzer service. Scroll to the bottom and click on Find Traces button to see traces. Use Ctrl-C in the terminal to exit the port-foward when you are done. Read more about Jaeger Grafana \u00b6 Create a secret which will be used to set the login credentials for Grafana cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: grafana namespace: istio-system type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Wait 2 minutes for the secret to be picked up and then launch the dashboard: istioctl dashboard grafana Log in using admin for both username and password. Navigate to the Istio Service Dashboard by clicking on the Home menu on the top left, then Istio, then Istio Service Dashboard. Select guestbook in the Service drop down. In a different tab, visit the guestbook application and refresh the page multiple times to generate some load, or run the load script you used previously. Switch back to the Grafana tab. Use Ctrl-C in the terminal to exit the port-foward when you are done. This Grafana dashboard provides metrics for each workload. Explore the other dashboard provided as well. Read more about Grafana . Prometheus \u00b6 Establish port forwarding from local port 9090 to the Prometheus instance. istioctl dashboard prometheus In the \u201cExpression\u201d input box, enter: istio_request_bytes_count . Click Execute and then select Graph. Then try another query: istio_requests_total{destination_service=\"guestbook.default.svc.cluster.local\", destination_version=\"2.0\"} Use Ctrl-C in the terminal to exit the port-foward when you are done. Kiali \u00b6 Kiali is an open-source project that installs on top of Istio to visualize your service mesh. It provides deeper insight into how your microservices interact with one another, and provides features such as circuit breakers and request rates for your services Create a secret which will be used to set the login credentials for Kiali cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kiali namespace: istio-system labels: app: kiali type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Establish port forwarding from local port 20001 to the Kiali instance. istioctl dashboard kiali Login with admin for both username and password. Select Graph and then choose default namespace. You should see a visual service graph of the various services in your Istio mesh. Use the Edge Labels dropdown and select Traffic rate per second to see the request rates as well. Kiali has a number of views to help you visualize your services. Click through the vairous tabs to explore the service graph, and the various views for workloads, applications, and services. Understand what happened \u00b6 Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Apps need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace. In the example, when a user visits the Guestbook app, the HTTP request is sent from the guestbook service to Watson Tone Analyzer. In order for the individual spans of guestbook service and Watson Tone Analyzer to be tied together, we have modified the guestbook service to extract the required headers (x-request-id, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context) and forward them onto the analyzer service when calling the analyzer service from the guestbook service. The change is in the v2/guestbook/main.go . By using the getForwardHeaders() method, we are able to extract the required headers, and then we use the required headers further when calling the analyzer service via the getPrimaryTone() method. Questions \u00b6 Does a user need to modify their app to get metrics for their apps? A: 1. Yes 2. No. (2 is correct) Does a user need to modify their app to get distributed tracing for their app to work properly? A: 1. Yes 2. No. (1 is correct) What distributed tracing system does Istio support by default? A: 1. Zipkin 2. Kibana 3. LogStash 4. Jaeger. (1 and 4 are correct) Continue to Exercise 5 - Expose the service mesh with the Istio Ingress Gateway \u00b6","title":"Lab 4. Observe service telemetry metrics"},{"location":"exercise-4/#exercise-4-observe-service-telemetry-metrics-and-tracing","text":"","title":"Exercise 4 - Observe service telemetry: metrics and tracing"},{"location":"exercise-4/#challenges-with-microservices","text":"We all know that microservice architecture is the perfect fit for cloud native applications and it increases the delivery velocities greatly. Envision you have many microservices that are delivered by multiple teams, how do you observe the the overall platform and each of the service to find out exactly what is going on with each of the services? When something goes wrong, how do you know which service or which communication among the few services are causing the problem?","title":"Challenges with microservices"},{"location":"exercise-4/#istio-telemetry","text":"Istio's tracing and metrics features are designed to provide broad and granular insight into the health of all services. Istio's role as a service mesh makes it the ideal data source for observability information, particularly in a microservices environment. As requests pass through multiple services, identifying performance bottlenecks becomes increasingly difficult using traditional debugging techniques. Distributed tracing provides a holistic view of requests transiting through multiple services, allowing for immediate identification of latency issues. With Istio, distributed tracing comes by default. This will expose latency, retry, and failure information for each hop in a request. You can read more about how Istio mixer enables telemetry reporting .","title":"Istio telemetry"},{"location":"exercise-4/#configure-istio-to-receive-telemetry-data","text":"Enable Istio monitoring dashboards, by running these two commands: kubectl patch cm managed-istio-custom -n ibm-operators --type = 'json' -p = '[{\"op\": \"add\", \"path\": \"/data/istio-monitoring\", \"value\":\"true\"}]' kubectl annotate iop -n ibm-operators managed-istio --overwrite version = \"custom-applied-at: $( date ) \" Verify that the Grafana, Prometheus, Kiali and Jaeger add-ons were installed successfully. All add-ons are installed into the istio-system namespace. kubectl get services -n istio-system Obtain the guestbook endpoint to access the guestbook. You can access the guestbook via the external IP for your service as guestbook is deployed as a load balancer service. Get the EXTERNAL-IP of the guestbook service via output below: kubectl get service guestbook -n default Go to this external ip address in the browser to try out your guestbook. This service will route you to either v1 or v2, at random. If you wish to see a different version, you'll need to do a hard refresh ( cmd + shift + r on a mac, or ctrl + f5 on a PC). Alternatively, you can curl the address. Generate a small load to the app, replacing guestbook_IP with the EXTERNAL-IP. for i in { 1 ..40 } ; do sleep 0 .2 ; curl -I http://<guestbook_IP>/ ; done","title":"Configure Istio to receive telemetry data"},{"location":"exercise-4/#view-guestbook-telemetry-data","text":"","title":"View guestbook telemetry data"},{"location":"exercise-4/#jaeger","text":"Launch the Jaeger dashboard: istioctl dashboard jaeger From the Services menu, select either the guestbook or analyzer service. Scroll to the bottom and click on Find Traces button to see traces. Use Ctrl-C in the terminal to exit the port-foward when you are done. Read more about Jaeger","title":"Jaeger"},{"location":"exercise-4/#grafana","text":"Create a secret which will be used to set the login credentials for Grafana cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: grafana namespace: istio-system type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Wait 2 minutes for the secret to be picked up and then launch the dashboard: istioctl dashboard grafana Log in using admin for both username and password. Navigate to the Istio Service Dashboard by clicking on the Home menu on the top left, then Istio, then Istio Service Dashboard. Select guestbook in the Service drop down. In a different tab, visit the guestbook application and refresh the page multiple times to generate some load, or run the load script you used previously. Switch back to the Grafana tab. Use Ctrl-C in the terminal to exit the port-foward when you are done. This Grafana dashboard provides metrics for each workload. Explore the other dashboard provided as well. Read more about Grafana .","title":"Grafana"},{"location":"exercise-4/#prometheus","text":"Establish port forwarding from local port 9090 to the Prometheus instance. istioctl dashboard prometheus In the \u201cExpression\u201d input box, enter: istio_request_bytes_count . Click Execute and then select Graph. Then try another query: istio_requests_total{destination_service=\"guestbook.default.svc.cluster.local\", destination_version=\"2.0\"} Use Ctrl-C in the terminal to exit the port-foward when you are done.","title":"Prometheus"},{"location":"exercise-4/#kiali","text":"Kiali is an open-source project that installs on top of Istio to visualize your service mesh. It provides deeper insight into how your microservices interact with one another, and provides features such as circuit breakers and request rates for your services Create a secret which will be used to set the login credentials for Kiali cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Secret metadata: name: kiali namespace: istio-system labels: app: kiali type: Opaque data: username: \"YWRtaW4=\" passphrase: \"YWRtaW4=\" EOF Establish port forwarding from local port 20001 to the Kiali instance. istioctl dashboard kiali Login with admin for both username and password. Select Graph and then choose default namespace. You should see a visual service graph of the various services in your Istio mesh. Use the Edge Labels dropdown and select Traffic rate per second to see the request rates as well. Kiali has a number of views to help you visualize your services. Click through the vairous tabs to explore the service graph, and the various views for workloads, applications, and services.","title":"Kiali"},{"location":"exercise-4/#understand-what-happened","text":"Although Istio proxies are able to automatically send spans, they need some hints to tie together the entire trace. Apps need to propagate the appropriate HTTP headers so that when the proxies send span information to Zipkin or Jaeger, the spans can be correlated correctly into a single trace. In the example, when a user visits the Guestbook app, the HTTP request is sent from the guestbook service to Watson Tone Analyzer. In order for the individual spans of guestbook service and Watson Tone Analyzer to be tied together, we have modified the guestbook service to extract the required headers (x-request-id, x-b3-traceid, x-b3-spanid, x-b3-parentspanid, x-b3-sampled, x-b3-flags, x-ot-span-context) and forward them onto the analyzer service when calling the analyzer service from the guestbook service. The change is in the v2/guestbook/main.go . By using the getForwardHeaders() method, we are able to extract the required headers, and then we use the required headers further when calling the analyzer service via the getPrimaryTone() method.","title":"Understand what happened"},{"location":"exercise-4/#questions","text":"Does a user need to modify their app to get metrics for their apps? A: 1. Yes 2. No. (2 is correct) Does a user need to modify their app to get distributed tracing for their app to work properly? A: 1. Yes 2. No. (1 is correct) What distributed tracing system does Istio support by default? A: 1. Zipkin 2. Kibana 3. LogStash 4. Jaeger. (1 and 4 are correct)","title":"Questions"},{"location":"exercise-4/#continue-to-exercise-5-expose-the-service-mesh-with-the-istio-ingress-gateway","text":"","title":"Continue to Exercise 5 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"exercise-5/","text":"Exercise 5 - Expose the service mesh with the Istio Ingress Gateway \u00b6 The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services. Expose the Guestbook app with Ingress Gateway \u00b6 Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../../plans kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1 Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name \u00b6 NLB host names are the DNS host names you can generate for each IBM Cloud Kubernetes deployment exposed with the Network LoadBalancer(NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service. References \u00b6 Kubernetes Ingress Istio Ingress Bring your own ALB Continue to Exercise 6 - Traffic Management \u00b6","title":"Lab 5. Expose the service mesh"},{"location":"exercise-5/#exercise-5-expose-the-service-mesh-with-the-istio-ingress-gateway","text":"The components deployed on the service mesh by default are not exposed outside the cluster. External access to individual services so far has been provided by creating an external load balancer or node port on each service. An Ingress Gateway resource can be created to allow external requests through the Istio Ingress Gateway to the backing services.","title":"Exercise 5 - Expose the service mesh with the Istio Ingress Gateway"},{"location":"exercise-5/#expose-the-guestbook-app-with-ingress-gateway","text":"Configure the guestbook default route with the Istio Ingress Gateway. The guestbook-gateway.yaml file is in this repository (istio101) in the workshop/plans directory. cd ../../plans kubectl create -f guestbook-gateway.yaml Get the EXTERNAL-IP of the Istio Ingress Gateway. kubectl get service istio-ingressgateway -n istio-system Output: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE istio-ingressgateway LoadBalancer 172 .21.254.53 169 .6.1.1 80 :31380/TCP,443:31390/TCP,31400:31400/TCP 1m 2d Make note of the external IP address that you retrieved in the previous step, as it will be used to access the Guestbook app in later parts of the course. Create an environment variable called $INGRESS_IP with your IP address. Example: export INGRESS_IP = 169 .6.1.1","title":"Expose the Guestbook app with Ingress Gateway"},{"location":"exercise-5/#connect-istio-ingress-gateway-to-the-ibm-cloud-kubernetes-service-nlb-host-name","text":"NLB host names are the DNS host names you can generate for each IBM Cloud Kubernetes deployment exposed with the Network LoadBalancer(NLB) service. These host names come with SSL certificate, the DNS registration, and health checks so you can benefit from them for any deployments that you expose via the NLB on IBM Cloud Kubernetes Service. You can run the IBM Cloud Kubernetes Service ALB, an API gateway of your choice, an Istio ingress gateway, and an MQTT server in parallel in your IBM Cloud Kubernetes Service cluster. Each one will have its own: 1. Publicly available wildcard host name 2. Wildcard SSL certificate associated with the host name 3. Health checks that you can configure if you use multizone deployments. Let's leverage this feature with Istio ingress gateway: Let's first check if you have any NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER If you haven't used this feature before, you will get an empty list. Obtain the Istio ingress gateway's external IP. Get the EXTERNAL-IP of the istio-ingressgateway service via output below: kubectl get service istio-ingressgateway -n istio-system Create the NLB host with the Istio ingress gateway's public IP address: ibmcloud ks nlb-dns create classic --cluster $MYCLUSTER --ip $INGRESS_IP List the NLB host names for your cluster: ibmcloud ks nlb-dnss --cluster $MYCLUSTER Example output: shell Retrieving host names, certificates, IPs, and health check monitors for network load balancer (NLB) pods in cluster <cluster_name>... OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud [\"169.1.1.1\"] None created mycluster-85f044fc29ce613c264409c04a76c95d-0001 Make note of the NLB host name ( ), as it will be used to access your Guestbook app in later parts of the course. Create an environment variable for it and test using curl or visit in your browser. Example: export NLB_HOSTNAME = mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud curl $NLB_HOSTNAME Enable health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor configure --cluster $MYCLUSTER --nlb-host $NLB_HOSTNAME --type HTTP --description \"Istio ingress gateway health check\" --path \"/healthz/ready\" --port 15021 --enable Monitor the health check of the NLB host for Istio ingress gateway: ibmcloud ks nlb-dns monitor status --cluster $MYCLUSTER After waiting for a bit, you should start to see the health monitor's status changed to Enabled. Example output: Retrieving health check monitor statuses for NLB pods... OK Hostname IP Health Monitor H.Monitor Status mycluster-85f044fc29ce613c264409c04a76c95d-0001.us-east.containers.appdomain.cloud 169 .1.1.1 Enabled Healthy Congratulations! You extended the base Ingress features by providing a DNS entry to the Istio service.","title":"Connect Istio Ingress Gateway to the IBM Cloud Kubernetes Service NLB Host Name"},{"location":"exercise-5/#references","text":"Kubernetes Ingress Istio Ingress Bring your own ALB","title":"References"},{"location":"exercise-5/#continue-to-exercise-6-traffic-management","text":"","title":"Continue to Exercise 6 - Traffic Management"},{"location":"exercise-6/","text":"Exercise 6 - Perform traffic management \u00b6 Using rules to manage traffic \u00b6 The core component used for traffic management in Istio is Pilot, which manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Each service consists of any number of instances running on pods, containers, VMs etc. Each service can have any number of versions (a.k.a. subsets). There can be distinct subsets of service instances running different variants of the app binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries. Virtual Services \u00b6 A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset or version of it) defined in the service registry. Destination Rules \u00b6 A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule . Service Entries \u00b6 A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access. The Guestbook app \u00b6 In the Guestbook app, there is one service: guestbook. The guestbook service has two distinct versions: the base version (version 1) and the modernized version (version 2). Each version of the service has three instances based on the number of replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, prior to creating any rules, Istio will route requests equally across version 1 and version 2 of the guestbook service and their respective instances in a round robin manner. However, new versions of a service can easily introduce bugs to the service mesh, so following A/B Testing and Canary Deployments is good practice. A/B testing with Istio \u00b6 A/B testing is a method of performing identical tests against two separate service versions in order to determine which performs better. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ): kubectl create -f guestbook-destination.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Next, apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". A subset or version of a route destination is identified with a reference to a named service subset which must be declared in a corresponding DestinationRule . Since there are three instances matching the criteria of hostname guestbook and subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. You can use the echo command to get this value, if you don't remember it. echo $NLB_HOSTNAME To enable the Istio service mesh for A/B testing against the new service version, modify the original VirtualService rule: kubectl replace -f virtualservice-test.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to the newer version of guestbook. All other requests fall-through to the next block, which routes all traffic to the original version of guestbook. Canary deployment \u00b6 In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to the modernized version 2 of guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to the newer version of the service. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the guestbook should swap between V1 or V2 at about the weight you specified. Route all traffic to v2 \u00b6 For the following exercises, we'll be working with Guestbook v2. Route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF Implementing circuit breakers with destination rules \u00b6 Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service. Further reading \u00b6 Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries Questions \u00b6 Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy Continue to Exercise 7 - Security \u00b6","title":"Lab 6. Perform traffic management"},{"location":"exercise-6/#exercise-6-perform-traffic-management","text":"","title":"Exercise 6 - Perform traffic management"},{"location":"exercise-6/#using-rules-to-manage-traffic","text":"The core component used for traffic management in Istio is Pilot, which manages and configures all the Envoy proxy instances deployed in a particular Istio service mesh. It lets you specify what rules you want to use to route traffic between Envoy proxies, which run as sidecars to each service in the mesh. Each service consists of any number of instances running on pods, containers, VMs etc. Each service can have any number of versions (a.k.a. subsets). There can be distinct subsets of service instances running different variants of the app binary. These variants are not necessarily different API versions. They could be iterative changes to the same service, deployed in different environments (prod, staging, dev, etc.). Pilot translates high-level rules into low-level configurations and distributes this config to Envoy instances. Pilot uses three types of configuration resources to manage traffic within its service mesh: Virtual Services, Destination Rules, and Service Entries.","title":"Using rules to manage traffic"},{"location":"exercise-6/#virtual-services","text":"A VirtualService defines a set of traffic routing rules to apply when a host is addressed. Each routing rule defines matching criteria for traffic of a specific protocol. If the traffic is matched, then it is sent to a named destination service (or subset or version of it) defined in the service registry.","title":"Virtual Services"},{"location":"exercise-6/#destination-rules","text":"A DestinationRule defines policies that apply to traffic intended for a service after routing has occurred. These rules specify configuration for load balancing, connection pool size from the sidecar, and outlier detection settings to detect and evict unhealthy hosts from the load balancing pool. Any destination host and subset referenced in a VirtualService rule must be defined in a corresponding DestinationRule .","title":"Destination Rules"},{"location":"exercise-6/#service-entries","text":"A ServiceEntry configuration enables services within the mesh to access a service not necessarily managed by Istio. The rule describes the endpoints, ports and protocols of a white-listed set of mesh-external domains and IP blocks that services in the mesh are allowed to access.","title":"Service Entries"},{"location":"exercise-6/#the-guestbook-app","text":"In the Guestbook app, there is one service: guestbook. The guestbook service has two distinct versions: the base version (version 1) and the modernized version (version 2). Each version of the service has three instances based on the number of replicas in guestbook-deployment.yaml and guestbook-v2-deployment.yaml . By default, prior to creating any rules, Istio will route requests equally across version 1 and version 2 of the guestbook service and their respective instances in a round robin manner. However, new versions of a service can easily introduce bugs to the service mesh, so following A/B Testing and Canary Deployments is good practice.","title":"The Guestbook app"},{"location":"exercise-6/#ab-testing-with-istio","text":"A/B testing is a method of performing identical tests against two separate service versions in order to determine which performs better. To prevent Istio from performing the default routing behavior between the original and modernized guestbook service, define the following rules (found in istio101/workshop/plans ): kubectl create -f guestbook-destination.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : DestinationRule metadata : name : destination-rule-guestbook spec : host : guestbook subsets : - name : v1 labels : version : '1.0' - name : v2 labels : version : '2.0' Next, apply the VirtualService kubectl replace -f virtualservice-all-v1.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 The VirtualService defines a rule that captures all HTTP traffic coming in through the Istio ingress gateway, guestbook-gateway , and routes 100% of the traffic to pods of the guestbook service with label \"version: v1\". A subset or version of a route destination is identified with a reference to a named service subset which must be declared in a corresponding DestinationRule . Since there are three instances matching the criteria of hostname guestbook and subset version: v1 , by default Envoy will send traffic to all three instances in a round robin manner. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. You can use the echo command to get this value, if you don't remember it. echo $NLB_HOSTNAME To enable the Istio service mesh for A/B testing against the new service version, modify the original VirtualService rule: kubectl replace -f virtualservice-test.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - match : - headers : user-agent : regex : '.*Firefox.*' route : - destination : host : guestbook subset : v2 - route : - destination : host : guestbook subset : v1 In Istio VirtualService rules, there can be only one rule for each service and therefore when defining multiple HTTPRoute blocks, the order in which they are defined in the yaml matters. Hence, the original VirtualService rule is modified rather than creating a new rule. With the modified rule, incoming requests originating from Firefox browsers will go to the newer version of guestbook. All other requests fall-through to the next block, which routes all traffic to the original version of guestbook.","title":"A/B testing with Istio"},{"location":"exercise-6/#canary-deployment","text":"In Canary Deployments , newer versions of services are incrementally rolled out to users to minimize the risk and impact of any bugs introduced by the newer version. To begin incrementally routing traffic to the newer version of the guestbook service, modify the original VirtualService rule: kubectl replace -f virtualservice-80-20.yaml Let's examine the rule: apiVersion : networking.istio.io/v1alpha3 kind : VirtualService metadata : name : virtual-service-guestbook spec : hosts : - '*' gateways : - guestbook-gateway http : - route : - destination : host : guestbook subset : v1 weight : 80 - destination : host : guestbook subset : v2 weight : 20 In the modified rule, the routed traffic is split between two different subsets of the guestbook service. In this manner, traffic to the modernized version 2 of guestbook is controlled on a percentage basis to limit the impact of any unforeseen bugs. This rule can be modified over time until eventually all traffic is directed to the newer version of the service. View the guestbook application using the $NLB_HOSTNAME specified in Exercise 5 and enter it as a URL in Firefox or Chrome web browsers. Ensure that you are using a hard refresh (command + Shift + R on Mac or Ctrl + F5 on windows) to remove any browser caching. You should notice that the guestbook should swap between V1 or V2 at about the weight you specified.","title":"Canary deployment"},{"location":"exercise-6/#route-all-traffic-to-v2","text":"For the following exercises, we'll be working with Guestbook v2. Route all traffic to guestbook v2 with a new VirtualService rule: cat <<EOF | kubectl replace -f - apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: virtual-service-guestbook spec: hosts: - '*' gateways: - guestbook-gateway http: - route: - destination: host: guestbook subset: v2 EOF","title":"Route all traffic to v2"},{"location":"exercise-6/#implementing-circuit-breakers-with-destination-rules","text":"Istio DestinationRules allow users to configure Envoy's implementation of circuit breakers . Circuit breakers are critical for defining the behavior for service-to-service communication in the service mesh. In the event of a failure for a particular service, circuit breakers allow users to set global defaults for failure recovery on a per service and/or per service version basis. Users can apply a traffic policy at the top level of the DestinationRule to create circuit breaker settings for an entire service, or it can be defined at the subset level to create settings for a particular version of a service. Depending on whether a service handles HTTP requests or TCP connections, DestinationRules expose a number of ways for Envoy to limit traffic to a particular service as well as define failure recovery behavior for services initiating the connection to an unhealthy service.","title":"Implementing circuit breakers with destination rules"},{"location":"exercise-6/#further-reading","text":"Istio Concept Istio Rules API Istio Proxy Debug Tool Traffic Management Circuit Breaking Timeouts and Retries","title":"Further reading"},{"location":"exercise-6/#questions","text":"Where are routing rules defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: VirtualService Where are service versions (subsets) defined? Options: (VirtualService, DestinationRule, ServiceEntry) Answer: DestinationRule Which Istio component is responsible for sending traffic management configurations to Istio sidecars? Options: (Mixer, Citadel, Pilot, Kubernetes) Answer: Pilot What is the name of the default proxy that runs in Istio sidecars and routes requests within the service mesh? Options: (NGINX, Envoy, HAProxy) Answer: Envoy","title":"Questions"},{"location":"exercise-6/#continue-to-exercise-7-security","text":"","title":"Continue to Exercise 7 - Security"},{"location":"exercise-7/","text":"Exercise 7 - Secure your services \u00b6 Mutual authentication with Transport Layer Security (mTLS) \u00b6 Istio can secure the communication between microservices without requiring application code changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When an application microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization). Enforce mTLS between all Istio services \u00b6 To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"istio-system\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically Configure access control for workloads using HTTP traffic \u00b6 Modify guestbook and analyzer deployments to use leverage the service accounts. Navigate to your guestbook dir first, for example: cd ../guestbook Add serviceaccount to your guestbook and analyzer deployments echo \" serviceAccountName: guestbook\" >> v1/guestbook-deployment.yaml echo \" serviceAccountName: guestbook\" >> v2/guestbook-deployment.yaml echo \" serviceAccountName: analyzer\" >> v2/analyzer-deployment.yaml redeploy the guestbook and analyzer deployments kubectl replace -f v1/guestbook-deployment.yaml kubectl replace -f v2/guestbook-deployment.yaml kubectl replace -f v2/analyzer-deployment.yaml Create a AuthorizationPolicy to disable all access to analyzer service. This will effectively not allow guestbook or any services to access it. cat <<EOF | kubectl create -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer EOF Output: authorizationpolicy.security.istio.io/analyzeraccess created Visit the Guestbook app from your favorite browser and validate that Guestbook V1 continues to work while Guestbook V2 will not run correctly. For every new message you write on the Guestbook v2 app, you will get a message such as \"Error - unable to detect Tone from the Analyzer service\". It can take up to 15 seconds for the change to propogate to the envoy sidecar(s) so you may not see the error right away. Configure the Analyzer service to only allow access from the Guestbook service using the added rules section: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer rules: - from: - source: principals: [\"cluster.local/ns/default/sa/guestbook\"] to: - operation: methods: [\"POST\"] EOF Visit the Guestbook app from your favorite browser and validate that Guestbook V2 works now. It can take a few seconds for the change to propogate to the envoy sidecar(s) so you may not observe Guestbook V2 to function right away. Cleanup \u00b6 Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook kubectl delete sa guestbook analyzer kubectl delete AuthorizationPolicy analyzeraccess Quiz \u00b6 True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False) Further Reading \u00b6 Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Lab 7. Secure your service mesh"},{"location":"exercise-7/#exercise-7-secure-your-services","text":"","title":"Exercise 7 - Secure your services"},{"location":"exercise-7/#mutual-authentication-with-transport-layer-security-mtls","text":"Istio can secure the communication between microservices without requiring application code changes. Security is provided by authenticating and encrypting communication paths within the cluster. This is becoming a common security and compliance requirement. Delegating communication security to Istio (as opposed to implementing TLS in each microservice), ensures that your application will be deployed with consistent and manageable security policies. Istio Citadel is an optional part of Istio's control plane components. When enabled, it provides each Envoy sidecar proxy with a strong (cryptographic) identity, in the form of a certificate. Identity is based on the microservice's service account and is independent of its specific network location, such as cluster or current IP address. Envoys then use the certificates to identify each other and establish an authenticated and encrypted communication channel between them. Citadel is responsible for: Providing each service with an identity representing its role. Providing a common trust root to allow Envoys to validate and authenticate each other. Providing a key management system, automating generation, distribution, and rotation of certificates and keys. When an application microservice connects to another microservice, the communication is redirected through the client side and server side Envoys. The end-to-end communication path is: Local TCP connection (i.e., localhost , not reaching the \"wire\") between the application and Envoy (client- and server-side); Mutually authenticated and encrypted connection between Envoy proxies. When Envoy proxies establish a connection, they exchange and validate certificates to confirm that each is indeed connected to a valid and expected peer. The established identities can later be used as basis for policy checks (e.g., access authorization).","title":"Mutual authentication with Transport Layer Security (mTLS)"},{"location":"exercise-7/#enforce-mtls-between-all-istio-services","text":"To enforce a mesh-wide authentication policy that requires mutual TLS, submit the following policy. This policy specifies that all workloads in the mesh will only accept encrypted requests using TLS. kubectl apply -f - <<EOF apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"istio-system\" spec: mtls: mode: STRICT EOF Visit your guestbook application by going to it in your browser. Everything should be working as expected! To confirm mTLS is infact enabled, you can run: istioctl x describe service guestbook Example output: Service : guestbook Port : http 80/HTTP targets pod port 3000 DestinationRule : destination-rule-guestbook for \"guestbook\" Matching subsets : v1,v2 No Traffic Policy Pod is STRICT, clients configured automatically","title":"Enforce mTLS between all Istio services"},{"location":"exercise-7/#configure-access-control-for-workloads-using-http-traffic","text":"Modify guestbook and analyzer deployments to use leverage the service accounts. Navigate to your guestbook dir first, for example: cd ../guestbook Add serviceaccount to your guestbook and analyzer deployments echo \" serviceAccountName: guestbook\" >> v1/guestbook-deployment.yaml echo \" serviceAccountName: guestbook\" >> v2/guestbook-deployment.yaml echo \" serviceAccountName: analyzer\" >> v2/analyzer-deployment.yaml redeploy the guestbook and analyzer deployments kubectl replace -f v1/guestbook-deployment.yaml kubectl replace -f v2/guestbook-deployment.yaml kubectl replace -f v2/analyzer-deployment.yaml Create a AuthorizationPolicy to disable all access to analyzer service. This will effectively not allow guestbook or any services to access it. cat <<EOF | kubectl create -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer EOF Output: authorizationpolicy.security.istio.io/analyzeraccess created Visit the Guestbook app from your favorite browser and validate that Guestbook V1 continues to work while Guestbook V2 will not run correctly. For every new message you write on the Guestbook v2 app, you will get a message such as \"Error - unable to detect Tone from the Analyzer service\". It can take up to 15 seconds for the change to propogate to the envoy sidecar(s) so you may not see the error right away. Configure the Analyzer service to only allow access from the Guestbook service using the added rules section: cat <<EOF | kubectl apply -f - apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: analyzeraccess spec: selector: matchLabels: app: analyzer rules: - from: - source: principals: [\"cluster.local/ns/default/sa/guestbook\"] to: - operation: methods: [\"POST\"] EOF Visit the Guestbook app from your favorite browser and validate that Guestbook V2 works now. It can take a few seconds for the change to propogate to the envoy sidecar(s) so you may not observe Guestbook V2 to function right away.","title":"Configure access control for workloads using HTTP traffic"},{"location":"exercise-7/#cleanup","text":"Run the following commands to clean up the Istio configuration resources as part of this exercise: kubectl delete PeerAuthentication default kubectl delete dr default kubectl delete dr destination-rule-guestbook kubectl delete sa guestbook analyzer kubectl delete AuthorizationPolicy analyzeraccess","title":"Cleanup"},{"location":"exercise-7/#quiz","text":"True or False? Istio Citadel provides each microservice with a strong, cryptographic, identity in the form of a certificate. The certificates' life cycle is fully managed by Istio. (True) Istio provides microservices with mutually authenticated connections, without requiring app code changes. (True) Mutual authentication must be on or off for the entire cluster, gradual adoption is not possible. (False)","title":"Quiz"},{"location":"exercise-7/#further-reading","text":"Basic TLS/SSL Terminology TLS Handshake Explained Istio Task Istio Concept","title":"Further Reading"}]}